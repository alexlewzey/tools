{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import enum\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import textwrap\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import anthropic\n",
    "import fasttext\n",
    "import google.generativeai as genai\n",
    "import lightgbm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F  # noqa: N812\n",
    "import vertexai\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    ")\n",
    "from torchvision import transforms\n",
    "from torchvision.models import ConvNeXt_Base_Weights, convnext_base\n",
    "from transformers import pipeline\n",
    "from typing_extensions import TypedDict\n",
    "from ultralytics import YOLO\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "from vertexai.generative_models import Image as VImage\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(linewidth=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available models\n",
    "\n",
    "links:\n",
    "- https://cloud.google.com/vertex-ai/generative-ai\n",
    "- https://aistudio.google.com/\n",
    "- https://platform.openai.com/playground/\n",
    "- https://console.anthropic.com/workbench\n",
    "- https://huggingface.co/models\n",
    "\n",
    "practical approach\n",
    "- start with closed models for POC for easy of use and performance\n",
    "- consider open source options if closed options don't perform on niche tasks or meet production requirements (cost, security etc)\n",
    "- HuggingFace is good place to start for open source models however best in class open source models not on HuggingFace to exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By task and open/closed:\n",
    "\n",
    "- image classification\n",
    "    - open\n",
    "        - TIMM (ViT, ResNet, ConvNext)\n",
    "        - HuggingFaceVision\n",
    "    - closed\n",
    "        - gemini 2\n",
    "        - openai\n",
    "        - claude 3\n",
    "- text to text\n",
    "    - open\n",
    "        - llama 3\n",
    "    - closed\n",
    "        - gemini 2\n",
    "        - openai\n",
    "- text+image to text\n",
    "    - open\n",
    "        - OpenCLIP\n",
    "        - llama 3\n",
    "    - closed\n",
    "        - gemini 2\n",
    "        - openai\n",
    "        - claude 3\n",
    "- image embeddings/similarity\n",
    "    - open\n",
    "        - TIMM (ViT, ResNet, ConvNext)\n",
    "        - [OpenCLIP](https://github.com/mlfoundations/open_clip)\n",
    "    - closed\n",
    "        - gemini 2\n",
    "        - openai\n",
    "- image captioning\n",
    "    - open\n",
    "        - Salesforce (BLIP)\n",
    "    - closed\n",
    "        - gemini 2\n",
    "        - openai\n",
    "        - claude vision\n",
    "- text embeddings\n",
    "    - open\n",
    "        - hugging face (sentence transformer, all-MiniLM-L6-v2)\n",
    "    - closed\n",
    "        - gemini 2\n",
    "        - openai\n",
    "- object detection/image segmentation\n",
    "    - open\n",
    "        - YOLO (You Only Look Once)\n",
    "        - Meta AI (Detectron2, Segment Anything Model i.e. SAM)\n",
    "        - MMDetection\n",
    "    - closed\n",
    "        - gemini 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "prompt: str = \"What is a cocker spaniel?\"\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-2.0-flash-exp\")\n",
    "r = model.generate_content(contents=prompt)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about recursion in programming.\"},\n",
    "    ],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Alice and Bob are going to a science fair on Friday.\",\n",
    "        },\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=1024,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, Claude\"}],\n",
    ")\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init()\n",
    "model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "chat_session = model.start_chat()\n",
    "r = chat_session.send_message(\"What is a cocker spaniel?\")\n",
    "text = r.candidates[0].content.parts[0].text\n",
    "print(\"\\n\".join(textwrap.wrap(text, 88)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text+image to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(Path.home() / \"Downloads\" / \"dogs.jpg\")\n",
    "img.thumbnail((1024,) * 2)\n",
    "prompt = \"What is this a picture of?\"\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"models/gemini-2.0-flash-exp\",\n",
    "    system_instruction=\"Answer in the style of Arnold Schwarzenegger\",\n",
    ")\n",
    "r = model.generate_content([prompt, img])\n",
    "print(\"\\n\".join(textwrap.wrap(r.text, width=88)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Choice(enum.Enum):\n",
    "    COCKER_SPANIEL = \"Cocker Spaniel\"\n",
    "    LABRADOR = \"Labrador\"\n",
    "    SPRINGER_SPANIEL = \"Springer Spaniel\"\n",
    "    PUG = \"Pug\"\n",
    "    GREAT_DANE = \"Great Dane\"\n",
    "\n",
    "\n",
    "class Dog(TypedDict):\n",
    "    breed: str\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"models/gemini-2.0-flash-exp\",\n",
    ")\n",
    "config = genai.GenerationConfig(\n",
    "    response_mime_type=\"application/json\", response_schema=list[Dog]\n",
    ")\n",
    "r = model.generate_content(\n",
    "    [\"What breed of dog is this?\", img], generation_config=config\n",
    ")\n",
    "pprint(json.loads(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "# images = Image.open(requests.get(url, stream=True).raw)\n",
    "# images.thumbnail((800,)*2)\n",
    "\n",
    "# import requests\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# from transformers import MllamaForConditionalGeneration, AutoProcessor,\n",
    "# BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,  # or load_in_4bit=True for 4-bit quantization\n",
    "# )\n",
    "# model = MllamaForConditionalGeneration.from_pretrained(\n",
    "#     model_id,\n",
    "#         quantization_config=quantization_config,\n",
    "#     # torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": [\n",
    "#         {\"type\": \"image\"},\n",
    "#      {\"type\": \"text\", \"text\": \"If I had to write a haiku for this one, it would be: \"}\n",
    "#     ]}\n",
    "# ]\n",
    "# input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# inputs = processor(\n",
    "#     images,\n",
    "#     input_text,\n",
    "#     add_special_tokens=False,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# output = model.generate(**inputs, max_new_tokens=30)\n",
    "# print(processor.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text + image to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\"\n",
    ")\n",
    "model.eval()\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "img_path = \"../../images/dogs.jpg\"\n",
    "images = Image.open(img_path)\n",
    "\n",
    "images = preprocess(images)[None,]\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    emb = model.encode_image(images)\n",
    "    embs = model.encode_text(text)\n",
    "    emb /= emb.norm(dim=-1, keepdim=True)\n",
    "    embs /= embs.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (1 * emb @ embs.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_embeddings(image_paths, batch_size=32):\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i : i + batch_size]\n",
    "        batch_images = []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            image = transform(image)\n",
    "            batch_images.append(image)\n",
    "\n",
    "        batch_tensor = torch.stack(batch_images)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch_tensor)\n",
    "            batch_embeddings = F.normalize(batch_embeddings.flatten(1), dim=1, p=2)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "\n",
    "img_path = \"../../images/dogs.jpg\"\n",
    "img_path_1 = \"../../images/dogs_1.jpg\"\n",
    "\n",
    "model = convnext_base(weights=ConvNeXt_Base_Weights.DEFAULT)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "embedding = get_batch_embeddings([img_path, img_path_1])\n",
    "cosine_similarity = embedding[[0]] @ embedding[[1]].T\n",
    "\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "img_path = \"../../images/dogs.jpg\"\n",
    "img_path_1 = \"../../images/dogs_1.jpg\"\n",
    "\n",
    "xs = [preprocess(Image.open(path)) for path in [img_path, img_path_1]]\n",
    "xs = torch.stack(xs)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    emb = model.encode_image(xs)\n",
    "    emb /= emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "cosine_similarity = emb[[0]] @ emb[[0]].T\n",
    "cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image captioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace (Salesforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"image-to-text\", model=\"Salesforce/blip-image-captioning-large\", use_fast=True\n",
    ")\n",
    "images = Image.open(img_path)\n",
    "r = pipe(images)\n",
    "\n",
    "print(r[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "img_path = \"../../images/dogs.jpg\"\n",
    "img = Image.open(img_path)\n",
    "img.thumbnail((1024,) * 2)\n",
    "\n",
    "prompt = \"Describe the contents of the image\"\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"models/gemini-2.0-flash-exp\",\n",
    ")\n",
    "r = model.generate_content([prompt, img])\n",
    "print(\"\\n\".join(textwrap.wrap(r.text, width=88)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../../images/dogs.jpg\"\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe the contents of the image\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "content = response.choices[0].message.content\n",
    "print(\"\\n\".join(textwrap.wrap(content, 88)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../../images/dogs.jpg\"\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/jpeg\",\n",
    "                        \"data\": base64_image,\n",
    "                    },\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"Describe the contents of the image\"},\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "text = message.content[0].text\n",
    "print(\"\\n\".join(textwrap.wrap(text, 88)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Part.from_image(VImage.load_from_file(\"../../images/dogs.jpg\"))\n",
    "model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "r = model.generate_content([image, \"Describe the contents of the image\"])\n",
    "print(\"\\n\".join(textwrap.wrap(r.text, 88)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Hello Mole!\", \"Hey Ted!!!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace (SentenceTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embs = torch.tensor(model.encode(texts))\n",
    "\n",
    "(embs[[0]] @ embs[[1]].T).item(), embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"BAAI/bge-large-en\")\n",
    "embs = torch.tensor(model.encode(texts))\n",
    "\n",
    "(embs[[0]] @ embs[[1]].T).item(), embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\"\n",
    ")\n",
    "model.eval()\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "text = tokenizer(texts)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    embs = model.encode_text(text)\n",
    "    embs /= embs.norm(dim=-1, keepdim=True)\n",
    "\n",
    "(embs[[0]] @ embs[[1]].T).item(), embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "r = genai.embed_content(model=\"models/text-embedding-004\", content=texts)\n",
    "embs = torch.tensor(r[\"embedding\"])\n",
    "(embs[[0]] @ embs[[1]].T).item(), embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "r = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\", input=texts, encoding_format=\"float\"\n",
    ")\n",
    "embs = torch.tensor([emb.embedding for emb in r.data])\n",
    "(embs[[0]] @ embs[[1]].T).item(), embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_explained_var(pca: PCA, fmt: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Return the variance captured by each principal component.\"\"\"\n",
    "    ratios = {\n",
    "        \"var\": pca.explained_variance_,\n",
    "        \"var_ratio\": pca.explained_variance_ratio_,\n",
    "        \"var_ratio_cum\": pca.explained_variance_ratio_.cumsum(),\n",
    "    }\n",
    "    df = pd.DataFrame(ratios)\n",
    "    if fmt:\n",
    "        df = pd.concat([df.iloc[:, 0], df.iloc[:, 1:].map(\"{:.1%}\".format)], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# def show_confusion_matrix(y_true, y_pred):\n",
    "#     cm = confusion_matrix(y_true, y_pred)\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=categories)\n",
    "#     _, ax = plt.subplots(figsize=(7, 7))\n",
    "#     disp.plot(ax=ax, xticks_rotation=90)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "def shannon_entropy(probabilities):\n",
    "    return -sum(p * math.log2(p) for p in probabilities if p > 0)\n",
    "\n",
    "\n",
    "def entropy(labels):\n",
    "    counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    probabilities = [count / total for count in counts.values()]\n",
    "    return shannon_entropy(probabilities)\n",
    "\n",
    "\n",
    "def feature_importance(names, importances) -> pd.DataFrame:\n",
    "    fi = pd.DataFrame({\"feature\": names, \"score\": importances}).sort_values(\n",
    "        \"score\", ascending=False\n",
    "    )\n",
    "    fi[\"pct\"] = fi.score / fi.score.sum()\n",
    "    return fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent\n",
    "# !kaggle datasets download -d deepshah16/song-lyrics-dataset\n",
    "# !unzip song-lyrics-dataset.zip\n",
    "paths = list((root_dir / \"tmp\" / \"csv\").iterdir())\n",
    "df = pd.concat(map(pd.read_csv, paths)).iloc[:, 1:]\n",
    "df.columns = df.columns.str.lower()\n",
    "df = df[~df.lyric.isnull()]\n",
    "df[\"n_char\"] = df.lyric.str.len()\n",
    "df[\"n_word\"] = df.lyric.str.split().str.len()\n",
    "df[\"n_unique_word\"] = df.lyric.str.split().apply(set).str.len()\n",
    "df[\"word_entropy\"] = df.lyric.str.split().apply(entropy)\n",
    "df[\"avg_word_len\"] = df.lyric.str.split().apply(lambda x: np.mean([len(i) for i in x]))\n",
    "df[\"label\"] = df[\"artist\"].astype(\"category\").cat.codes\n",
    "categories = df.artist.astype(\"category\").cat.categories.tolist()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    df.lyric.tolist(), df.label, test_size=0.2, random_state=42\n",
    ")\n",
    "st = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "st_transform = FunctionTransformer(st.encode)\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    st_transform,\n",
    "    lightgbm.LGBMClassifier(verbose=-100),\n",
    ")\n",
    "pipe.fit(x_train, y_train)\n",
    "yhat = pipe.predict(x_valid)\n",
    "print(accuracy_score(y_valid, yhat), f1_score(y_valid, yhat, average=\"micro\"))\n",
    "# show_confusion_matrix(y_valid, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_features = [\n",
    "    \"n_char\",\n",
    "    \"n_word\",\n",
    "    \"n_unique_word\",\n",
    "    \"word_entropy\",\n",
    "    \"avg_word_len\",\n",
    "]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df, df.label, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "feature_union = FeatureUnion(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            make_pipeline(\n",
    "                FunctionTransformer(lambda x: x[\"lyric\"], validate=False),\n",
    "                TfidfVectorizer(),\n",
    "            ),\n",
    "        ),\n",
    "        (\"manual\", FunctionTransformer(lambda x: x[manual_features], validate=False)),\n",
    "    ]\n",
    ")\n",
    "pipe = make_pipeline(feature_union, lightgbm.LGBMClassifier(verbose=-100, n_jobs=-1))\n",
    "pipe.fit(x_train, y_train)\n",
    "yhat = pipe.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "f1 = f1_score(y_test, yhat, average=\"micro\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "# show_confusion_matrix(y_valid, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pipe.named_steps[\"lgbmclassifier\"]\n",
    "vec_columns = (\n",
    "    pipe.named_steps[\"featureunion\"]\n",
    "    .transformer_list[0][1]\n",
    "    .named_steps[\"tfidfvectorizer\"]\n",
    "    .get_feature_names_out()\n",
    ")\n",
    "features = vec_columns.tolist() + manual_features\n",
    "fi = feature_importance(features, m.feature_importances_)\n",
    "fig, ax = plt.subplots(figsize=(13, 5.5))\n",
    "fi.head(40).iloc[::-1].plot(kind=\"barh\", x=\"feature\", y=\"pct\", ax=ax)\n",
    "ax.tick_params(labelsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"lyric\"]\n",
    "y = df[\"label\"].astype(str)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_fasttext_data(texts, labels, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for text, label in zip(texts, labels, strict=False):\n",
    "            f.write(f\"__label__{label} {text}\\n\")\n",
    "\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False, suffix=\".txt\") as temp_train:\n",
    "    prepare_fasttext_data(x_train, y_train, temp_train.name)\n",
    "    train_file = temp_train.name\n",
    "\n",
    "model = fasttext.train_supervised(input=train_file, lr=1.0, epoch=25, wordNgrams=2)\n",
    "\n",
    "\n",
    "def predict(model, texts):\n",
    "    return [model.predict(text)[0][0].replace(\"__label__\", \"\") for text in texts]\n",
    "\n",
    "\n",
    "y_pred = predict(model, x_valid)\n",
    "y_valid = y_valid.astype(str)\n",
    "y_pred = [str(pred) for pred in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "f1 = f1_score(y_valid, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "os.unlink(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## object detection/image segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.generate_content(\n",
    "    [\n",
    "        (\n",
    "            \"Return a bounding box for each of the objects in this image in \"\n",
    "            \"[ymin, xmin, ymax, xmax] format.\"\n",
    "        ),\n",
    "        images,\n",
    "    ],\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        response_mime_type=\"application/json\", response_schema=list[list[int]]\n",
    "    ),\n",
    ")\n",
    "json.loads(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO (ultralytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLO(\"yolo11n.pt\")\n",
    "img_path = \"../../images/dogs.jpg\"\n",
    "images = Image.open(img_path)\n",
    "images.thumbnail((512,) * 2)\n",
    "images.save(img_path)\n",
    "r = yolo(img_path)\n",
    "r[0].show()\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(yolo.names[i[5]], i[4]) for i in r[0].boxes.data.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(r[0].boxes.data)):\n",
    "    # Get box coordinates, confidence, and class\n",
    "    box_data = r[0].boxes.data[i]\n",
    "    x1, y1, x2, y2 = box_data[0:4]\n",
    "    conf = box_data[4]\n",
    "    cls = box_data[5]\n",
    "\n",
    "    # Convert to integers for cropping\n",
    "    x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "    # Crop image\n",
    "    cropped = images.crop((x1, y1, x2, y2))\n",
    "cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"A chocolate brown short-haired cocker spaniel\",\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"standard\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "print(response.data[0].url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab probs * prediction index\n",
    "probs = np.random.dirichlet(np.ones(5), 10)\n",
    "y = probs.argmax(1)\n",
    "probs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complexity if all predictions are correct\n",
    "complexity = np.exp(-np.log(probs[range(len(probs)), y]).mean())\n",
    "complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complexity if actual labels are random\n",
    "y_rand = np.random.randint(10)\n",
    "complexity = np.exp(-np.log(probs[range(len(probs)), y_rand]).mean())\n",
    "complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter efficient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1\n",
    "weights = np.random.random((10, 10))\n",
    "a = np.random.random((10, rank))\n",
    "b = np.random.random((rank, 10))\n",
    "assert (a @ b).shape == weights.shape\n",
    "weights, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total parameters, decomposed parameters\n",
    "weights.flatten().shape[0], len(a) + len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = 0.1\n",
    "weights_new = weights + (a @ b) * scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"cat\": np.array([1, 0, 0, 0]),\n",
    "    \"dog\": np.array([0, 1, 0, 0]),\n",
    "    \"is\": np.array([0, 0, 1, 0]),\n",
    "    \"black\": np.array([0, 0, 0, 1]),\n",
    "}\n",
    "sentence = [\"cat\", \"is\", \"black\"]\n",
    "input_embeddings = np.array([vocab[word] for word in sentence])\n",
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prompt_tokens = 2\n",
    "prompt_embeddings = np.random.randn(num_prompt_tokens, 4)\n",
    "combined = np.vstack([prompt_embeddings, input_embeddings])\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**32 / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass and update the virtual tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
